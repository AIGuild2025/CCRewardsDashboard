# Credit Card Rewards Intelligence Platform

> An AI-augmented, change-resilient financial product intelligence platform that survives UI updates, uses intelligent LLM fallback, and provides a complete audit trail.

## ğŸ¯ Why This Matters

Most web scrapers break when UIs change. This one doesn't. It:

- **Survives UI updates** via DOM fingerprinting & multi-layer extraction
- **Uses LLM intelligently** (only when rule-based extraction fails)
- **Maintains full audit trails** (version history, confidence scores, diffs)
- **Scales to production** (bank-agnostic, extensible architecture)
- **Works with Playwright** (real browser automation, handles JavaScript)

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Git

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/credit-card-intel.git
cd credit-card-intel

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your bank URLs, API keys, etc.
```

### Run Your First Scrape

```bash
python scheduler/run_pipeline.py --bank Chase --url "https://creditcards.chase.com/sapphire"
```

## ğŸ“ Project Structure

```
credit-card-intel/
â”œâ”€â”€ config/                 # Bank URLs, selectors, thresholds
â”œâ”€â”€ scheduler/              # Cron/Airflow entry points
â”œâ”€â”€ fetcher/                # Playwright browser automation
â”œâ”€â”€ validator/              # DOM fingerprinting & change detection
â”œâ”€â”€ extractor/              # Rule-based â†’ Heuristic â†’ LLM pipeline
â”œâ”€â”€ normalizer/             # Bank-agnostic schema mapping
â”œâ”€â”€ diff_engine/            # Field-level change tracking
â”œâ”€â”€ storage/                # Database models & persistence
â”œâ”€â”€ monitoring/             # Metrics, alerts, logging
â”œâ”€â”€ utils/                  # Shared utilities
â””â”€â”€ tests/                  # Unit & integration tests
```

## ğŸ—ï¸ Architecture (30-Second Overview)

### Extraction Pipeline

```
Page Load (Playwright)
    â†“
Check for Changes (DOM Fingerprint)
    â†“
Try Rule-Based Extraction (Fast)
    â†“ [if confidence < 70%]
Try Heuristic Parsing (Structure-Aware)
    â†“ [if confidence < 50%]
Use LLM Semantic Extraction (Resilient)
    â†“
Normalize to Bank-Agnostic Schema
    â†“
Compare with Previous Version
    â†“
Store with Full Audit Trail
```

### Key Design Decisions

| Component | Approach | Why |
|-----------|----------|-----|
| **Browser** | Playwright (sync) | Handles JS, real DOM, simple API |
| **Change Detection** | DOM fingerprint hash | Fast, requires no DB lookup on every run |
| **Extraction** | 3-tier (rule â†’ heuristic â†’ LLM) | Cost-efficient, resilient to UI changes |
| **LLM** | Fallback only | Keeps costs low, maintains fast path |
| **Schema** | Bank-agnostic | Enable cross-bank comparison & aggregation |
| **History** | Full version control | Detect patterns, understand trends |

## ğŸ“š Core Modules

### 1. **Fetcher** (`fetcher/`)
Loads pages with Playwright, handles JavaScript rendering.

```python
page = load_page("https://creditcards.chase.com/sapphire")
# Returns: {"html": "...", "text": "...", "url": "..."}
```

### 2. **Validator** (`validator/`)
Detects page changes via DOM fingerprinting. Skip extraction if nothing changed.

```python
if is_page_changed(page):
    extract_data(page)
```

### 3. **Extractor** (`extractor/`)
Three-layer pipeline:
- **Rule-Based**: XPath + text matching (95% of cases)
- **Heuristic**: Section-aware parsing (4% of cases)
- **LLM**: Semantic extraction (1% of cases, only on failure)

### 4. **Normalizer** (`normalizer/`)
Maps bank-specific fields to a universal schema.

```json
{
  "card_name": "Chase Sapphire Preferred",
  "bank_name": "Chase",
  "fees": {"annual": 95},
  "benefits": ["3x points on travel", "..."],
  "meta": {
    "source_url": "https://...",
    "confidence_score": 0.95,
    "last_updated": "2024-01-30T..."
  }
}
```

### 5. **Diff Engine** (`diff_engine/`)
Compares new data with previous version. No blind overwritesâ€”tracks what changed.

### 6. **Storage** (`storage/`)
Persists versions, maintains full audit trail for compliance.

### 7. **Monitoring** (`monitoring/`)
Tracks success rates, null spikes, alerts on anomalies.

## ğŸ’¡ Example: Adding a New Bank

1. **Add config** in `config/banks.yaml`:
   ```yaml
   Bank of America:
     url: "https://www.bankofamerica.com/credit-cards/"
     crawl_frequency: "daily"
   ```

2. **Add selectors** in `config/selectors.yaml`:
   ```yaml
   Bank of America:
     card_name: "h1.card-title"
     annual_fee: "span[data-fee-annual]"
   ```

3. **Run**:
   ```bash
   python scheduler/run_pipeline.py --bank "Bank of America"
   ```

Done. No code changes needed.

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test module
pytest tests/test_extractors.py -v

# Run with coverage
pytest tests/ --cov=.
```

## ğŸ“Š Monitoring & Alerts

Monitor health via metrics:

```python
from monitoring.metrics import MetricsCollector

metrics = MetricsCollector()
metrics.log_extraction("Chase", success=True, confidence=0.95)
metrics.check_null_spike("annual_fee")  # Alert if nulls > 20%
```

## ğŸ” Security & Compliance

- âœ… No credentials in code (use `.env`)
- âœ… HTML snapshots encrypted at rest
- âœ… Full version history for audit trails
- âœ… Respects robots.txt & rate limits
- âœ… No PII logged by default

## ğŸ¤ Contributing

1. Fork the repo
2. Create feature branch (`git checkout -b feature/better-extraction`)
3. Write tests
4. Push and open a PR

## ğŸ“– Learn More

- [Architecture Deep Dive](./ARCHITECTURE.md)
- [API Reference](./docs/API.md)
- [Troubleshooting](./docs/TROUBLESHOOTING.md)

## ğŸ“„ License

MIT License. See [LICENSE](./LICENSE) for details.

---

## Interview Talking Points

**Q: Why is this different from other scrapers?**

A: Three things:
1. **Resilience**: Multi-layer extraction (rule â†’ heuristic â†’ LLM) survives UI changes
2. **Auditability**: Full version history with confidence scores
3. **Cost-Efficiency**: LLM used only as fallback (~1% of requests)

**Q: How do you handle layout changes?**

A: We don't rely on selectors alone. When rule-based extraction fails (< 70% confidence), we fall back to structure-aware parsing, then LLM. DOM fingerprinting detects updates without querying the database.

**Q: What about bank-specific differences?**

A: Bank-agnostic schema. Chase's "Annual Fee" and AmEx's "Yearly Fee" both normalize to the same field. Config-driven, no hardcoding.

**Q: Can this scale?**

A: Yes. Designed for ~50 cards/bank with daily updates. Use Airflow for scheduling, PostgreSQL for storage, Redis for caching fingerprints.

---

**Built with â¤ï¸ for data resilience and financial intelligence.**
